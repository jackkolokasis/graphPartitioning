OpenJDK 64-Bit Server VM warning: Max heap size too large for Compressed Oops
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
19/06/12 16:41:41 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 3957763@sith0.cluster.ics.forth.gr
19/06/12 16:41:41 INFO SignalUtils: Registered signal handler for TERM
19/06/12 16:41:41 INFO SignalUtils: Registered signal handler for HUP
19/06/12 16:41:41 INFO SignalUtils: Registered signal handler for INT
19/06/12 16:41:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/06/12 16:41:42 INFO SecurityManager: Changing view acls to: kolokasis
19/06/12 16:41:42 INFO SecurityManager: Changing modify acls to: kolokasis
19/06/12 16:41:42 INFO SecurityManager: Changing view acls groups to: 
19/06/12 16:41:42 INFO SecurityManager: Changing modify acls groups to: 
19/06/12 16:41:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kolokasis); groups with view permissions: Set(); users  with modify permissions: Set(kolokasis); groups with modify permissions: Set()
19/06/12 16:41:42 INFO TransportClientFactory: Successfully created connection to /192.168.2.107:36952 after 48 ms (0 ms spent in bootstraps)
19/06/12 16:41:42 INFO SecurityManager: Changing view acls to: kolokasis
19/06/12 16:41:42 INFO SecurityManager: Changing modify acls to: kolokasis
19/06/12 16:41:42 INFO SecurityManager: Changing view acls groups to: 
19/06/12 16:41:42 INFO SecurityManager: Changing modify acls groups to: 
19/06/12 16:41:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kolokasis); groups with view permissions: Set(); users  with modify permissions: Set(kolokasis); groups with modify permissions: Set()
19/06/12 16:41:42 INFO TransportClientFactory: Successfully created connection to /192.168.2.107:36952 after 1 ms (0 ms spent in bootstraps)
19/06/12 16:41:42 INFO DiskBlockManager: Created local directory at /tmp/spark-70b13155-424a-44c1-9197-c62c0d2d2406/executor-eece12bf-f292-46bd-bada-1ce7bb1ac57b/blockmgr-d905333f-d5fa-4bf1-bf60-2aa4f45eb9ad
19/06/12 16:41:42 INFO MemoryStore: MemoryStore started with capacity 23.2 GB
19/06/12 16:41:42 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@192.168.2.107:36952
19/06/12 16:41:42 INFO WorkerWatcher: Connecting to worker spark://Worker@192.168.2.100:34840
19/06/12 16:41:42 INFO TransportClientFactory: Successfully created connection to /192.168.2.100:34840 after 1 ms (0 ms spent in bootstraps)
19/06/12 16:41:42 INFO WorkerWatcher: Successfully connected to spark://Worker@192.168.2.100:34840
19/06/12 16:41:43 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
19/06/12 16:41:43 INFO Executor: Starting executor ID 10 on host 192.168.2.100
19/06/12 16:41:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40035.
19/06/12 16:41:43 INFO NettyBlockTransferService: Server created on 192.168.2.100:40035
19/06/12 16:41:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/06/12 16:41:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(10, 192.168.2.100, 40035, None)
19/06/12 16:41:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(10, 192.168.2.100, 40035, None)
19/06/12 16:41:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(10, 192.168.2.100, 40035, None)
19/06/12 16:41:46 INFO CoarseGrainedExecutorBackend: Got assigned task 51
19/06/12 16:41:46 INFO CoarseGrainedExecutorBackend: Got assigned task 67
19/06/12 16:41:46 INFO CoarseGrainedExecutorBackend: Got assigned task 83
19/06/12 16:41:46 INFO CoarseGrainedExecutorBackend: Got assigned task 99
19/06/12 16:41:46 INFO CoarseGrainedExecutorBackend: Got assigned task 114
19/06/12 16:41:46 INFO Executor: Running task 70.0 in stage 0.0 (TID 83)
19/06/12 16:41:46 INFO Executor: Running task 49.0 in stage 0.0 (TID 67)
19/06/12 16:41:46 INFO Executor: Running task 110.0 in stage 0.0 (TID 114)
19/06/12 16:41:46 INFO Executor: Running task 28.0 in stage 0.0 (TID 51)
19/06/12 16:41:46 INFO Executor: Running task 91.0 in stage 0.0 (TID 99)
19/06/12 16:41:46 INFO Executor: Fetching spark://192.168.2.107:36952/jars/spark-examples_2.11-2.2.0-SNAPSHOT.jar with timestamp 1560346900493
19/06/12 16:41:46 INFO TransportClientFactory: Successfully created connection to /192.168.2.107:36952 after 4 ms (0 ms spent in bootstraps)
19/06/12 16:41:46 INFO Utils: Fetching spark://192.168.2.107:36952/jars/spark-examples_2.11-2.2.0-SNAPSHOT.jar to /tmp/spark-70b13155-424a-44c1-9197-c62c0d2d2406/executor-eece12bf-f292-46bd-bada-1ce7bb1ac57b/spark-700ef23c-e283-4daf-91e1-f06ee63e8a67/fetchFileTemp7111159898645833580.tmp
19/06/12 16:41:46 INFO Utils: Copying /tmp/spark-70b13155-424a-44c1-9197-c62c0d2d2406/executor-eece12bf-f292-46bd-bada-1ce7bb1ac57b/spark-700ef23c-e283-4daf-91e1-f06ee63e8a67/9172785461560346900493_cache to /opt/spark/spark-master/work/app-20190612164140-0000/10/./spark-examples_2.11-2.2.0-SNAPSHOT.jar
19/06/12 16:41:46 INFO Executor: Adding file:/opt/spark/spark-master/work/app-20190612164140-0000/10/./spark-examples_2.11-2.2.0-SNAPSHOT.jar to class loader
19/06/12 16:41:47 INFO TorrentBroadcast: Started reading broadcast variable 1
19/06/12 16:41:47 INFO TransportClientFactory: Successfully created connection to /192.168.2.103:46106 after 1 ms (0 ms spent in bootstraps)
19/06/12 16:41:47 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 23.2 GB)
19/06/12 16:41:47 INFO TorrentBroadcast: Reading broadcast variable 1 took 182 ms
19/06/12 16:41:47 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.1 KB, free 23.2 GB)
19/06/12 16:41:47 INFO HadoopRDD: Input split: hdfs://sith0-hadoop:9000/user/kolokasis/soc-sinaweibo.txt:1360503058+31639606
19/06/12 16:41:47 INFO HadoopRDD: Input split: hdfs://sith0-hadoop:9000/user/kolokasis/soc-sinaweibo.txt:2024934784+31639606
19/06/12 16:41:47 INFO HadoopRDD: Input split: hdfs://sith0-hadoop:9000/user/kolokasis/soc-sinaweibo.txt:2689366510+31639606
19/06/12 16:41:47 INFO HadoopRDD: Input split: hdfs://sith0-hadoop:9000/user/kolokasis/soc-sinaweibo.txt:4018229962+31639693
19/06/12 16:41:47 INFO TorrentBroadcast: Started reading broadcast variable 0
19/06/12 16:41:47 INFO HadoopRDD: Input split: hdfs://sith0-hadoop:9000/user/kolokasis/soc-sinaweibo.txt:3290519024+31639606
19/06/12 16:41:47 INFO TransportClientFactory: Successfully created connection to /192.168.2.103:46791 after 1 ms (0 ms spent in bootstraps)
19/06/12 16:41:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 23.2 GB)
19/06/12 16:41:47 INFO TorrentBroadcast: Reading broadcast variable 0 took 39 ms
19/06/12 16:41:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 374.2 KB, free 23.2 GB)
19/06/12 16:41:48 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.10.executor.filesystem.file.largeRead_ops
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:122)
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.10.executor.filesystem.file.read_bytes
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:122)
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.10.executor.filesystem.file.read_ops
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:122)
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.10.executor.filesystem.file.write_bytes
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:122)
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.10.executor.filesystem.file.write_ops
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:122)
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.10.executor.filesystem.hdfs.largeRead_ops
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:122)
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.10.executor.filesystem.hdfs.read_bytes
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:122)
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.10.executor.filesystem.hdfs.read_ops
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:122)
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.10.executor.filesystem.hdfs.write_bytes
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:12