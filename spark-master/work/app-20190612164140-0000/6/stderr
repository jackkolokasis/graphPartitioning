OpenJDK 64-Bit Server VM warning: Max heap size too large for Compressed Oops
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
19/06/12 16:41:41 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 3957761@sith0.cluster.ics.forth.gr
19/06/12 16:41:41 INFO SignalUtils: Registered signal handler for TERM
19/06/12 16:41:41 INFO SignalUtils: Registered signal handler for HUP
19/06/12 16:41:41 INFO SignalUtils: Registered signal handler for INT
19/06/12 16:41:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/06/12 16:41:42 INFO SecurityManager: Changing view acls to: kolokasis
19/06/12 16:41:42 INFO SecurityManager: Changing modify acls to: kolokasis
19/06/12 16:41:42 INFO SecurityManager: Changing view acls groups to: 
19/06/12 16:41:42 INFO SecurityManager: Changing modify acls groups to: 
19/06/12 16:41:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kolokasis); groups with view permissions: Set(); users  with modify permissions: Set(kolokasis); groups with modify permissions: Set()
19/06/12 16:41:42 INFO TransportClientFactory: Successfully created connection to /192.168.2.107:36952 after 61 ms (0 ms spent in bootstraps)
19/06/12 16:41:42 INFO SecurityManager: Changing view acls to: kolokasis
19/06/12 16:41:42 INFO SecurityManager: Changing modify acls to: kolokasis
19/06/12 16:41:42 INFO SecurityManager: Changing view acls groups to: 
19/06/12 16:41:42 INFO SecurityManager: Changing modify acls groups to: 
19/06/12 16:41:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kolokasis); groups with view permissions: Set(); users  with modify permissions: Set(kolokasis); groups with modify permissions: Set()
19/06/12 16:41:42 INFO TransportClientFactory: Successfully created connection to /192.168.2.107:36952 after 1 ms (0 ms spent in bootstraps)
19/06/12 16:41:42 INFO DiskBlockManager: Created local directory at /tmp/spark-70b13155-424a-44c1-9197-c62c0d2d2406/executor-eece12bf-f292-46bd-bada-1ce7bb1ac57b/blockmgr-e35f45e5-7376-456e-83eb-4232a5995d7a
19/06/12 16:41:42 INFO MemoryStore: MemoryStore started with capacity 23.2 GB
19/06/12 16:41:42 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@192.168.2.107:36952
19/06/12 16:41:42 INFO WorkerWatcher: Connecting to worker spark://Worker@192.168.2.100:34840
19/06/12 16:41:42 INFO TransportClientFactory: Successfully created connection to /192.168.2.100:34840 after 1 ms (0 ms spent in bootstraps)
19/06/12 16:41:42 INFO WorkerWatcher: Successfully connected to spark://Worker@192.168.2.100:34840
19/06/12 16:41:42 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
19/06/12 16:41:42 INFO Executor: Starting executor ID 6 on host 192.168.2.100
19/06/12 16:41:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41795.
19/06/12 16:41:42 INFO NettyBlockTransferService: Server created on 192.168.2.100:41795
19/06/12 16:41:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/06/12 16:41:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(6, 192.168.2.100, 41795, None)
19/06/12 16:41:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(6, 192.168.2.100, 41795, None)
19/06/12 16:41:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(6, 192.168.2.100, 41795, None)
19/06/12 16:41:46 INFO CoarseGrainedExecutorBackend: Got assigned task 56
19/06/12 16:41:46 INFO CoarseGrainedExecutorBackend: Got assigned task 72
19/06/12 16:41:46 INFO CoarseGrainedExecutorBackend: Got assigned task 88
19/06/12 16:41:46 INFO CoarseGrainedExecutorBackend: Got assigned task 103
19/06/12 16:41:46 INFO CoarseGrainedExecutorBackend: Got assigned task 118
19/06/12 16:41:46 INFO Executor: Running task 76.0 in stage 0.0 (TID 88)
19/06/12 16:41:46 INFO Executor: Running task 55.0 in stage 0.0 (TID 72)
19/06/12 16:41:46 INFO Executor: Running task 34.0 in stage 0.0 (TID 56)
19/06/12 16:41:46 INFO Executor: Running task 115.0 in stage 0.0 (TID 118)
19/06/12 16:41:46 INFO Executor: Running task 96.0 in stage 0.0 (TID 103)
19/06/12 16:41:46 INFO Executor: Fetching spark://192.168.2.107:36952/jars/spark-examples_2.11-2.2.0-SNAPSHOT.jar with timestamp 1560346900493
19/06/12 16:41:46 INFO TransportClientFactory: Successfully created connection to /192.168.2.107:36952 after 2 ms (0 ms spent in bootstraps)
19/06/12 16:41:46 INFO Utils: Fetching spark://192.168.2.107:36952/jars/spark-examples_2.11-2.2.0-SNAPSHOT.jar to /tmp/spark-70b13155-424a-44c1-9197-c62c0d2d2406/executor-eece12bf-f292-46bd-bada-1ce7bb1ac57b/spark-ede121da-0e37-48ee-9e93-78b9fcd242df/fetchFileTemp8713483297331125218.tmp
19/06/12 16:41:46 INFO Utils: Copying /tmp/spark-70b13155-424a-44c1-9197-c62c0d2d2406/executor-eece12bf-f292-46bd-bada-1ce7bb1ac57b/spark-ede121da-0e37-48ee-9e93-78b9fcd242df/9172785461560346900493_cache to /opt/spark/spark-master/work/app-20190612164140-0000/6/./spark-examples_2.11-2.2.0-SNAPSHOT.jar
19/06/12 16:41:46 INFO Executor: Adding file:/opt/spark/spark-master/work/app-20190612164140-0000/6/./spark-examples_2.11-2.2.0-SNAPSHOT.jar to class loader
19/06/12 16:41:47 INFO TorrentBroadcast: Started reading broadcast variable 1
19/06/12 16:41:47 INFO TransportClientFactory: Successfully created connection to /192.168.2.101:45329 after 1 ms (0 ms spent in bootstraps)
19/06/12 16:41:47 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 23.2 GB)
19/06/12 16:41:47 INFO TorrentBroadcast: Reading broadcast variable 1 took 134 ms
19/06/12 16:41:47 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.1 KB, free 23.2 GB)
19/06/12 16:41:47 INFO HadoopRDD: Input split: hdfs://sith0-hadoop:9000/user/kolokasis/soc-sinaweibo.txt:3448717054+31639606
19/06/12 16:41:47 INFO HadoopRDD: Input split: hdfs://sith0-hadoop:9000/user/kolokasis/soc-sinaweibo.txt:2214772420+31639606
19/06/12 16:41:47 INFO HadoopRDD: Input split: hdfs://sith0-hadoop:9000/user/kolokasis/soc-sinaweibo.txt:885908968+31639606
19/06/12 16:41:47 INFO HadoopRDD: Input split: hdfs://sith0-hadoop:9000/user/kolokasis/soc-sinaweibo.txt:2847564540+31639606
19/06/12 16:41:47 INFO HadoopRDD: Input split: hdfs://sith0-hadoop:9000/user/kolokasis/soc-sinaweibo.txt:1550340694+31639606
19/06/12 16:41:47 INFO TorrentBroadcast: Started reading broadcast variable 0
19/06/12 16:41:47 INFO TransportClientFactory: Successfully created connection to /192.168.2.102:33975 after 1 ms (0 ms spent in bootstraps)
19/06/12 16:41:48 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.6.executor.filesystem.file.largeRead_ops
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:122)
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.6.executor.filesystem.file.read_bytes
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:122)
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.6.executor.filesystem.file.read_ops
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:122)
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.6.executor.filesystem.file.write_bytes
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:122)
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.6.executor.filesystem.file.write_ops
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:122)
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.6.executor.filesystem.hdfs.largeRead_ops
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:122)
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.6.executor.filesystem.hdfs.read_bytes
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:122)
2)
