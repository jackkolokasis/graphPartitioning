OpenJDK 64-Bit Server VM warning: Max heap size too large for Compressed Oops
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
19/06/12 16:41:41 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 3957764@sith0.cluster.ics.forth.gr
19/06/12 16:41:41 INFO SignalUtils: Registered signal handler for TERM
19/06/12 16:41:41 INFO SignalUtils: Registered signal handler for HUP
19/06/12 16:41:41 INFO SignalUtils: Registered signal handler for INT
19/06/12 16:41:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/06/12 16:41:42 INFO SecurityManager: Changing view acls to: kolokasis
19/06/12 16:41:42 INFO SecurityManager: Changing modify acls to: kolokasis
19/06/12 16:41:42 INFO SecurityManager: Changing view acls groups to: 
19/06/12 16:41:42 INFO SecurityManager: Changing modify acls groups to: 
19/06/12 16:41:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kolokasis); groups with view permissions: Set(); users  with modify permissions: Set(kolokasis); groups with modify permissions: Set()
19/06/12 16:41:42 INFO TransportClientFactory: Successfully created connection to /192.168.2.107:36952 after 54 ms (0 ms spent in bootstraps)
19/06/12 16:41:42 INFO SecurityManager: Changing view acls to: kolokasis
19/06/12 16:41:42 INFO SecurityManager: Changing modify acls to: kolokasis
19/06/12 16:41:42 INFO SecurityManager: Changing view acls groups to: 
19/06/12 16:41:42 INFO SecurityManager: Changing modify acls groups to: 
19/06/12 16:41:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(kolokasis); groups with view permissions: Set(); users  with modify permissions: Set(kolokasis); groups with modify permissions: Set()
19/06/12 16:41:42 INFO TransportClientFactory: Successfully created connection to /192.168.2.107:36952 after 1 ms (0 ms spent in bootstraps)
19/06/12 16:41:42 INFO DiskBlockManager: Created local directory at /tmp/spark-70b13155-424a-44c1-9197-c62c0d2d2406/executor-eece12bf-f292-46bd-bada-1ce7bb1ac57b/blockmgr-f3f4dbea-734e-4391-9f3d-c4ee3df7842f
19/06/12 16:41:42 INFO MemoryStore: MemoryStore started with capacity 23.2 GB
19/06/12 16:41:42 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@192.168.2.107:36952
19/06/12 16:41:42 INFO WorkerWatcher: Connecting to worker spark://Worker@192.168.2.100:34840
19/06/12 16:41:42 INFO TransportClientFactory: Successfully created connection to /192.168.2.100:34840 after 1 ms (0 ms spent in bootstraps)
19/06/12 16:41:42 INFO WorkerWatcher: Successfully connected to spark://Worker@192.168.2.100:34840
19/06/12 16:41:42 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
19/06/12 16:41:42 INFO Executor: Starting executor ID 11 on host 192.168.2.100
19/06/12 16:41:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44895.
19/06/12 16:41:43 INFO NettyBlockTransferService: Server created on 192.168.2.100:44895
19/06/12 16:41:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/06/12 16:41:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(11, 192.168.2.100, 44895, None)
19/06/12 16:41:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(11, 192.168.2.100, 44895, None)
19/06/12 16:41:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(11, 192.168.2.100, 44895, None)
19/06/12 16:41:46 INFO CoarseGrainedExecutorBackend: Got assigned task 47
19/06/12 16:41:46 INFO CoarseGrainedExecutorBackend: Got assigned task 63
19/06/12 16:41:46 INFO CoarseGrainedExecutorBackend: Got assigned task 79
19/06/12 16:41:46 INFO CoarseGrainedExecutorBackend: Got assigned task 95
19/06/12 16:41:46 INFO CoarseGrainedExecutorBackend: Got assigned task 110
19/06/12 16:41:46 INFO Executor: Running task 105.0 in stage 0.0 (TID 110)
19/06/12 16:41:46 INFO Executor: Running task 64.0 in stage 0.0 (TID 79)
19/06/12 16:41:46 INFO Executor: Running task 43.0 in stage 0.0 (TID 63)
19/06/12 16:41:46 INFO Executor: Running task 85.0 in stage 0.0 (TID 95)
19/06/12 16:41:46 INFO Executor: Running task 23.0 in stage 0.0 (TID 47)
19/06/12 16:41:46 INFO Executor: Fetching spark://192.168.2.107:36952/jars/spark-examples_2.11-2.2.0-SNAPSHOT.jar with timestamp 1560346900493
19/06/12 16:41:46 INFO TransportClientFactory: Successfully created connection to /192.168.2.107:36952 after 2 ms (0 ms spent in bootstraps)
19/06/12 16:41:46 INFO Utils: Fetching spark://192.168.2.107:36952/jars/spark-examples_2.11-2.2.0-SNAPSHOT.jar to /tmp/spark-70b13155-424a-44c1-9197-c62c0d2d2406/executor-eece12bf-f292-46bd-bada-1ce7bb1ac57b/spark-e098f472-b884-4a88-9264-ffb57fb44362/fetchFileTemp7577215064787083261.tmp
19/06/12 16:41:46 INFO Utils: Copying /tmp/spark-70b13155-424a-44c1-9197-c62c0d2d2406/executor-eece12bf-f292-46bd-bada-1ce7bb1ac57b/spark-e098f472-b884-4a88-9264-ffb57fb44362/9172785461560346900493_cache to /opt/spark/spark-master/work/app-20190612164140-0000/11/./spark-examples_2.11-2.2.0-SNAPSHOT.jar
19/06/12 16:41:46 INFO Executor: Adding file:/opt/spark/spark-master/work/app-20190612164140-0000/11/./spark-examples_2.11-2.2.0-SNAPSHOT.jar to class loader
19/06/12 16:41:47 INFO TorrentBroadcast: Started reading broadcast variable 1
19/06/12 16:41:47 INFO TransportClientFactory: Successfully created connection to /192.168.2.102:44532 after 1 ms (0 ms spent in bootstraps)
19/06/12 16:41:47 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.3 KB, free 23.2 GB)
19/06/12 16:41:47 INFO TorrentBroadcast: Reading broadcast variable 1 took 389 ms
19/06/12 16:41:47 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.1 KB, free 23.2 GB)
19/06/12 16:41:47 INFO HadoopRDD: Input split: hdfs://sith0-hadoop:9000/user/kolokasis/soc-sinaweibo.txt:2499528874+31639606
19/06/12 16:41:47 INFO HadoopRDD: Input split: hdfs://sith0-hadoop:9000/user/kolokasis/soc-sinaweibo.txt:1835097148+31639606
19/06/12 16:41:47 INFO HadoopRDD: Input split: hdfs://sith0-hadoop:9000/user/kolokasis/soc-sinaweibo.txt:1170665422+31639606
19/06/12 16:41:47 INFO HadoopRDD: Input split: hdfs://sith0-hadoop:9000/user/kolokasis/soc-sinaweibo.txt:537873302+31639606
19/06/12 16:41:47 INFO TorrentBroadcast: Started reading broadcast variable 0
19/06/12 16:41:47 INFO HadoopRDD: Input split: hdfs://sith0-hadoop:9000/user/kolokasis/soc-sinaweibo.txt:3132320994+31639606
19/06/12 16:41:47 INFO TransportClientFactory: Successfully created connection to /192.168.2.102:33188 after 1 ms (0 ms spent in bootstraps)
19/06/12 16:41:48 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.11.executor.filesystem.file.largeRead_ops
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:122)
19/06/12 16:41:48 WARN CsvReporter: Error writing to app-20190612164140-0000.11.executor.filesystem.file.read_bytes
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:241)
	at com.codahale.metrics.CsvReporter.reportGauge(CsvReporter.java:234)
	at com.codahale.metrics.CsvReporter.report(CsvReporter.java:150)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:162)
	at org.apache.spark.metrics.sink.CsvSink.report(CsvSink.scala:71)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at org.apache.spark.metrics.MetricsSystem$$anonfun$report$1.apply(MetricsSystem.scala:116)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.metrics.MetricsSystem.report(MetricsSystem.scala:116)
	at org.apache.spark.executor.Executor.stop(Executor.scala:199)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1$$anon$1.run(CoarseGrainedExecutorBackend.scala:122)
2)
